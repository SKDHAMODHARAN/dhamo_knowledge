# 12 â€” Production Best Practices

## The Difference Between "It Works" and "It's Production-Ready"

```text
  "It works on my laptop"  â‰   "It can handle real users"

  DEV:
    âœ… 1 replica, no health checks, no limits, :latest tag, root user
    "Look, it runs!"

  PRODUCTION:
    âœ… Multiple replicas across zones
    âœ… Health checks (liveness + readiness + startup)
    âœ… Resource requests AND limits
    âœ… Pinned image versions
    âœ… Non-root container
    âœ… RBAC, NetworkPolicies, PodSecurityStandards
    âœ… Monitoring, alerting, logging
    âœ… Auto-scaling (HPA + Cluster Autoscaler)
    âœ… PodDisruptionBudgets
    âœ… Graceful shutdown handling
    âœ… Rolling update strategy
    "It can survive Black Friday traffic and a 2 AM node failure"
```

---

## The Production-Ready Deployment (Complete Example)

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: web-app
  namespace: production
  labels:
    app: web-app
    version: "2.5.0"
    team: platform
spec:
  replicas: 3
  revisionHistoryLimit: 5

  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxSurge: 1
      maxUnavailable: 0

  selector:
    matchLabels:
      app: web-app

  template:
    metadata:
      labels:
        app: web-app
        version: "2.5.0"
    spec:
      serviceAccountName: web-app-sa
      automountServiceAccountToken: false

      securityContext:
        runAsNonRoot: true
        runAsUser: 1000
        fsGroup: 2000

      affinity:
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
            - weight: 100
              podAffinityTerm:
                labelSelector:
                  matchLabels:
                    app: web-app
                topologyKey: kubernetes.io/hostname

      topologySpreadConstraints:
        - maxSkew: 1
          topologyKey: topology.kubernetes.io/zone
          whenUnsatisfiable: DoNotSchedule
          labelSelector:
            matchLabels:
              app: web-app

      terminationGracePeriodSeconds: 30

      containers:
        - name: app
          image: registry.company.com/web-app:2.5.0    # Pinned version!
          imagePullPolicy: IfNotPresent

          ports:
            - name: http
              containerPort: 8080
              protocol: TCP

          resources:
            requests:
              cpu: "250m"
              memory: "256Mi"
            limits:
              cpu: "1"
              memory: "512Mi"

          securityContext:
            allowPrivilegeEscalation: false
            readOnlyRootFilesystem: true
            capabilities:
              drop:
                - ALL

          startupProbe:
            httpGet:
              path: /healthz
              port: http
            failureThreshold: 30
            periodSeconds: 10

          livenessProbe:
            httpGet:
              path: /healthz
              port: http
            initialDelaySeconds: 0
            periodSeconds: 10
            timeoutSeconds: 3
            failureThreshold: 3

          readinessProbe:
            httpGet:
              path: /ready
              port: http
            initialDelaySeconds: 0
            periodSeconds: 5
            timeoutSeconds: 3
            failureThreshold: 3

          envFrom:
            - configMapRef:
                name: web-app-config
            - secretRef:
                name: web-app-secrets

          volumeMounts:
            - name: tmp
              mountPath: /tmp

      volumes:
        - name: tmp
          emptyDir: {}
```

---

## Checklist: Go/No-Go for Production

### ğŸ—ï¸ Deployment Configuration

| Item | Why | Status |
|------|-----|--------|
| `replicas >= 2` | Single replica = single point of failure | â˜ |
| `maxUnavailable: 0` | Never have fewer than desired during updates | â˜ |
| `revisionHistoryLimit` set | Control how many old ReplicaSets are kept | â˜ |
| Image tag is pinned version (NOT `:latest`) | Know exactly what's running | â˜ |
| `imagePullPolicy: IfNotPresent` | Avoid pulling on every restart | â˜ |
| Use private container registry | Control what runs in your cluster | â˜ |

### ğŸ’Š Health & Resilience

| Item | Why | Status |
|------|-----|--------|
| Liveness probe configured | Auto-restart deadlocked containers | â˜ |
| Readiness probe configured | Don't send traffic to unhealthy Pods | â˜ |
| Startup probe for slow-starting apps | Give time to boot before checking | â˜ |
| `terminationGracePeriodSeconds` set | Allow graceful shutdown | â˜ |
| PodDisruptionBudget created | Prevent mass eviction during maintenance | â˜ |
| Pod anti-affinity rules | Spread Pods across nodes | â˜ |
| Topology spread across zones | Survive availability zone failure | â˜ |

### ğŸ“¦ Resource Management

| Item | Why | Status |
|------|-----|--------|
| CPU request set | Scheduler knows how much to reserve | â˜ |
| Memory request set | Scheduler knows how much to reserve | â˜ |
| CPU limit set | Prevent CPU starvation of other Pods | â˜ |
| Memory limit set | Prevent OOM killing other Pods | â˜ |
| HPA configured | Handle traffic spikes automatically | â˜ |
| `minReplicas >= 2` in HPA | Always have redundancy | â˜ |
| `maxReplicas` capped | Prevent runaway scaling (and bills) | â˜ |

### ğŸ”’ Security

| Item | Why | Status |
|------|-----|--------|
| `runAsNonRoot: true` | Prevent root container escape | â˜ |
| `readOnlyRootFilesystem: true` | Prevent filesystem tampering | â˜ |
| `allowPrivilegeEscalation: false` | Container can't gain privileges | â˜ |
| Drop ALL capabilities | Minimize Linux kernel access | â˜ |
| Dedicated ServiceAccount (not default) | Least-privilege API access | â˜ |
| `automountServiceAccountToken: false` (if not needed) | Don't expose token unnecessarily | â˜ |
| NetworkPolicy restricting traffic | Only allow needed communication | â˜ |
| Pod Security Standard: `restricted` | Enforce security at namespace level | â˜ |
| Secrets not in Git | Use external secrets management | â˜ |
| Secrets encrypted at rest in etcd | Protect stored secrets | â˜ |

### ğŸ“Š Observability

| Item | Why | Status |
|------|-----|--------|
| Structured (JSON) logging | Machine-parseable, queryable | â˜ |
| Centralized log collection | Don't rely on `kubectl logs` | â˜ |
| Prometheus metrics exposed | Monitor application health | â˜ |
| Grafana dashboard created | Visualize app metrics | â˜ |
| Alerts configured for errors | Get notified before users do | â˜ |
| Alerts configured for latency | Catch performance degradation | â˜ |
| Alert routing (Slack/PagerDuty) | Right people notified | â˜ |

### ğŸ§¹ Operational Readiness

| Item | Why | Status |
|------|-----|--------|
| CI/CD pipeline deploys via Helm | Repeatable, auditable deploys | â˜ |
| Rollback procedure documented | Fast recovery from bad deploys | â˜ |
| etcd backup automated | Recover from cluster failure | â˜ |
| Cluster Autoscaler configured | Handle infrastructure scaling | â˜ |
| Resource quotas per namespace | Prevent one team from hogging cluster | â˜ |
| Disaster recovery plan | What if the entire cluster goes down? | â˜ |

---

## Graceful Shutdown â€” A Deep Dive

```text
When K8s kills a Pod (during scale-down, update, or node drain):

  1. Pod marked for termination
  2. Pod removed from Service endpoints (no new traffic)
  3. SIGTERM sent to container (POLITE: "please shut down")
  4. Container has terminationGracePeriodSeconds to finish work
  5. If not stopped â†’ SIGKILL (FORCED: "you're done NOW")

  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚                                                              â”‚
  â”‚  Timeline:                                                   â”‚
  â”‚                                                              â”‚
  â”‚  t=0s    SIGTERM sent                                        â”‚
  â”‚  t=0-30s Container should:                                   â”‚
  â”‚          â”œâ”€â”€ Stop accepting new requests                     â”‚
  â”‚          â”œâ”€â”€ Finish processing current requests              â”‚
  â”‚          â”œâ”€â”€ Close database connections                      â”‚
  â”‚          â””â”€â”€ Flush logs and metrics                          â”‚
  â”‚  t=30s   If still running â†’ SIGKILL (forced kill)            â”‚
  â”‚                                                              â”‚
  â”‚  YOUR APP MUST HANDLE SIGTERM!                               â”‚
  â”‚  Many frameworks do this automatically (Express, Spring, etc.)â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Lifecycle Hooks

```yaml
containers:
  - name: app
    image: my-app:2.5.0
    lifecycle:
      preStop:
        exec:
          command: ["sh", "-c", "sleep 5"]
```

**Why `sleep 5`?** There's a race condition: the endpoint removal (step 2) and SIGTERM (step 3) happen in parallel. The `sleep 5` gives kube-proxy time to update routing rules before your app starts shutting down.

---

## Resource Quotas â€” Prevent Cluster Abuse

```yaml
apiVersion: v1
kind: ResourceQuota
metadata:
  name: team-quota
  namespace: team-alpha
spec:
  hard:
    requests.cpu: "20"          # Total CPU requests across all Pods
    requests.memory: "40Gi"
    limits.cpu: "40"
    limits.memory: "80Gi"
    pods: "50"                  # Max 50 Pods in this namespace
    services: "10"
    persistentvolumeclaims: "20"

---
# LimitRange â€” set defaults for Pods that don't specify resources
apiVersion: v1
kind: LimitRange
metadata:
  name: default-limits
  namespace: team-alpha
spec:
  limits:
    - type: Container
      default:                  # Default limits (if not specified)
        cpu: "500m"
        memory: "256Mi"
      defaultRequest:           # Default requests (if not specified)
        cpu: "250m"
        memory: "128Mi"
      max:                      # Maximum any container can request
        cpu: "4"
        memory: "8Gi"
      min:                      # Minimum any container must request
        cpu: "50m"
        memory: "64Mi"
```

---

## Multi-Tenancy â€” Namespace-Per-Team Strategy

```text
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚                        CLUSTER                               â”‚
  â”‚                                                              â”‚
  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
  â”‚  â”‚ ns: team-alpha â”‚  â”‚ ns: team-beta  â”‚  â”‚ ns: platform   â”‚   â”‚
  â”‚  â”‚               â”‚  â”‚               â”‚  â”‚               â”‚   â”‚
  â”‚  â”‚ ResourceQuota â”‚  â”‚ ResourceQuota â”‚  â”‚ ResourceQuota â”‚   â”‚
  â”‚  â”‚ NetworkPolicy â”‚  â”‚ NetworkPolicy â”‚  â”‚ NetworkPolicy â”‚   â”‚
  â”‚  â”‚ LimitRange    â”‚  â”‚ LimitRange    â”‚  â”‚ LimitRange    â”‚   â”‚
  â”‚  â”‚ RBAC (team)   â”‚  â”‚ RBAC (team)   â”‚  â”‚ RBAC (team)   â”‚   â”‚
  â”‚  â”‚ PSS: restrict â”‚  â”‚ PSS: restrict â”‚  â”‚ PSS: baseline â”‚   â”‚
  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
  â”‚                                                              â”‚
  â”‚  Each team gets:                                             â”‚
  â”‚  â”œâ”€â”€ Their own namespace                                     â”‚
  â”‚  â”œâ”€â”€ Resource quotas (can't hog the cluster)                â”‚
  â”‚  â”œâ”€â”€ RBAC (can only access their namespace)                 â”‚
  â”‚  â”œâ”€â”€ NetworkPolicies (isolated from other teams)            â”‚
  â”‚  â””â”€â”€ Pod Security Standards (enforced)                      â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## CI/CD Pipeline Pattern

```text
  Developer pushes code
         â”‚
         â–¼
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚   CI          â”‚
  â”‚  1. Lint code â”‚
  â”‚  2. Run tests â”‚
  â”‚  3. Build     â”‚
  â”‚     Docker    â”‚
  â”‚     image     â”‚
  â”‚  4. Push to   â”‚
  â”‚     registry  â”‚
  â”‚  5. Scan for  â”‚
  â”‚     vulns     â”‚
  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚   CD          â”‚
  â”‚  6. helm      â”‚
  â”‚     upgrade   â”‚
  â”‚     --install â”‚
  â”‚     --atomic  â”‚
  â”‚  7. Run smoke â”‚
  â”‚     tests     â”‚
  â”‚  8. Monitor   â”‚
  â”‚     for 5 min â”‚
  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚  If errors:   â”‚
  â”‚  helm rollbackâ”‚
  â”‚  Alert team   â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Key CI/CD Principles

| Principle | Why |
|-----------|-----|
| Build once, deploy everywhere | Same image for dev/staging/prod |
| Use `--atomic` flag | Auto-rollback if deploy fails |
| Scan images for vulnerabilities | Catch CVEs before production |
| Run smoke tests after deploy | Verify basic functionality |
| Use GitOps (ArgoCD/FluxCD) | Git is the source of truth for cluster state |

---

## Cost Optimization

```text
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚  COST OPTIMIZATION STRATEGIES                                â”‚
  â”‚                                                              â”‚
  â”‚  1. RIGHT-SIZE resources (VPA recommendations)               â”‚
  â”‚     Don't allocate 4 CPU if you use 0.2 CPU                  â”‚
  â”‚                                                              â”‚
  â”‚  2. USE SPOT/PREEMPTIBLE INSTANCES for non-critical          â”‚
  â”‚     70-90% cheaper, but can be reclaimed                     â”‚
  â”‚                                                              â”‚
  â”‚  3. CLUSTER AUTOSCALER scales nodes down when idle           â”‚
  â”‚     Don't pay for empty nodes                                â”‚
  â”‚                                                              â”‚
  â”‚  4. SET RESOURCE QUOTAS per namespace                        â”‚
  â”‚     Prevent teams from over-provisioning                     â”‚
  â”‚                                                              â”‚
  â”‚  5. USE HPA to match capacity to demand                     â”‚
  â”‚     Scale down when traffic drops                            â”‚
  â”‚                                                              â”‚
  â”‚  6. REVIEW UNUSED RESOURCES regularly                        â”‚
  â”‚     Orphaned PVs, unused LoadBalancers, idle namespaces      â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## What's Next?

â¡ï¸ **[13 â€” Troubleshooting](./13-troubleshooting.md)** â€” Debug like a pro when things break
